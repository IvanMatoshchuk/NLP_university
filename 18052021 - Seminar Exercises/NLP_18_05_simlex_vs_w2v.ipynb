{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redo simlex999 word pair similarity rating exercise with w2v models, e.g. wiki.en.vec and/or your own home-grown model (via gensim or fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web-public corpus (1M) from https://wortschatz.uni-leipzig.de/en/download/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng-com_web-public_2018_1M-sentences.txt', 'r') as file:\n",
    "    corpus = file.read().lower().replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sent = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(sentence):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return phrase\n",
    "\n",
    "extended_corpus = [decontracted(sentence) for sentence in corpus_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r\"[?|$|&|*|%|@|(|)|~]\"\n",
    "        filtered_sentence = re.sub(PATTERN, r\"\", sentence)\n",
    "    else:\n",
    "        PATTERN = r\"[^a-zA-Z ]\"\n",
    "        filtered_sentence = re.sub(PATTERN, r\"\", sentence)\n",
    "    return filtered_sentence.strip()\n",
    "\n",
    "cleaned_corpus = [remove_characters_before_tokenization(sentence) for sentence in extended_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "corpus_w_stopwords = [remove_stopwords(sentence) for sentence in cleaned_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_final = [word_tokenize(sentence) for sentence in corpus_w_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://fasttext.cc/docs/en/crawl-vectors.html ... trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"cc.en.300.vec\") # wiki.en.vec\n",
    "model_2 = gensim.models.Word2Vec(corpus_final, size=300, window=10, min_count=10, sample=1e-3) # my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>join</td>\n",
       "      <td>acquire</td>\n",
       "      <td>V</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>send</td>\n",
       "      <td>attend</td>\n",
       "      <td>V</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.70</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>gather</td>\n",
       "      <td>attend</td>\n",
       "      <td>V</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>absorb</td>\n",
       "      <td>withdraw</td>\n",
       "      <td>V</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>attend</td>\n",
       "      <td>arrive</td>\n",
       "      <td>V</td>\n",
       "      <td>6.08</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  \\\n",
       "0       old          new   A       1.58      2.72      2.81      2   \n",
       "1     smart  intelligent   A       9.20      1.75      2.46      1   \n",
       "2      hard    difficult   A       8.77      3.76      2.21      2   \n",
       "3     happy     cheerful   A       9.55      2.56      2.34      1   \n",
       "4      hard         easy   A       0.95      3.76      2.07      2   \n",
       "..      ...          ...  ..        ...       ...       ...    ...   \n",
       "994    join      acquire   V       2.85      2.86      2.93      2   \n",
       "995    send       attend   V       1.67      2.70      3.17      2   \n",
       "996  gather       attend   V       4.80      2.75      3.17      2   \n",
       "997  absorb     withdraw   V       2.97      3.11      3.04      2   \n",
       "998  attend       arrive   V       6.08      3.17      3.22      2   \n",
       "\n",
       "     Assoc(USF)  SimAssoc333  SD(SimLex)  \n",
       "0          7.25            1        0.41  \n",
       "1          7.11            1        0.67  \n",
       "2          5.94            1        1.19  \n",
       "3          5.85            1        2.18  \n",
       "4          5.82            1        0.93  \n",
       "..          ...          ...         ...  \n",
       "994        0.00            0        0.99  \n",
       "995        0.00            0        1.44  \n",
       "996        0.00            0        1.97  \n",
       "997        0.00            0        1.75  \n",
       "998        0.00            0        1.18  \n",
       "\n",
       "[999 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "simlex999 = pd.read_csv(\"SimLex-999.txt\", sep=\"\\t\")\n",
    "simlex999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v wiki score:  (0.48450447967244925, 1.818804540998229e-59)\n",
      "my model score:  (0.2923378271374335, 5.599965145998254e-21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1bcb3af33b4c>:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  wiki_score = model.wv.similarity(x, y)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "simlex_scores = []\n",
    "w2v_wiki_scores = []\n",
    "my_model_scores = []\n",
    "for x, y, simlex_score in zip(simlex999.word1, simlex999.word2, simlex999.SimLex999):\n",
    "    try:\n",
    "        wiki_score = model.wv.similarity(x, y)\n",
    "        my_model_score = model_2.wv.similarity(x, y)\n",
    "        if my_model_score == \"None\":\n",
    "            raise ValueError(\"None\")\n",
    "            \n",
    "        simlex_scores.append(simlex_score)\n",
    "        w2v_wiki_scores.append(wiki_score)\n",
    "        my_model_scores.append(my_model_score)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print(\"w2v wiki score: \",stats.pearsonr(w2v_wiki_scores,simlex_scores))\n",
    "print(\"my model score: \",stats.pearsonr(my_model_scores,simlex_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
