{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seminar2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_zCm8o6T7Zu"
      },
      "source": [
        "import string\n",
        "import random\n",
        "import time\n",
        "from typing import List"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGB91mkDUGG-"
      },
      "source": [
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    :param text: Takes input sentence\n",
        "    :return: tokenized sentence\n",
        "    \"\"\"\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, ' '+punct+' ')\n",
        "    t = text.split()\n",
        "    return t"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zjp-7R1UPBA"
      },
      "source": [
        "def get_ngrams(n: int, tokens: list) -> list:\n",
        "    \"\"\"\n",
        "    :param n: n-gram size\n",
        "    :param tokens: tokenized sentence\n",
        "    :return: list of ngrams\n",
        "    ngrams of tuple form: ((previous wordS!), target word)\n",
        "    \"\"\"\n",
        "    # tokens.append('<END>')\n",
        "    tokens = (n-1)*['<START>']+tokens\n",
        "    l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n",
        "    return l"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9NKUEQUxWv",
        "outputId": "1914ac4f-3872-4e20-9fb3-308e953ec1f0"
      },
      "source": [
        "class NgramModel(object):\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "\n",
        "        # dictionary that keeps list of candidate words given context\n",
        "        self.context = {}\n",
        "\n",
        "        # keeps track of how many times ngram has appeared in the text before\n",
        "        self.ngram_counter = {}\n",
        "\n",
        "    def update(self, sentence: str) -> None:\n",
        "        \"\"\"\n",
        "        Updates Language Model\n",
        "        :param sentence: input text\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        ngrams = get_ngrams(n, tokenize(sentence))\n",
        "        for ngram in ngrams:\n",
        "            if ngram in self.ngram_counter:\n",
        "                self.ngram_counter[ngram] += 1.0\n",
        "            else:\n",
        "                self.ngram_counter[ngram] = 1.0\n",
        "\n",
        "            prev_words, target_word = ngram\n",
        "            if prev_words in self.context:\n",
        "                self.context[prev_words].append(target_word)\n",
        "            else:\n",
        "                self.context[prev_words] = [target_word]\n",
        "\n",
        "    def prob(self, context, token):\n",
        "        \"\"\"\n",
        "        Calculates probability of a candidate token to be generated given a context\n",
        "        :return: conditional probability\n",
        "        \"\"\"\n",
        "        try:\n",
        "            count_of_token = self.ngram_counter[(context, token)]\n",
        "            count_of_context = float(len(self.context[context]))\n",
        "            result = count_of_token / count_of_context\n",
        "\n",
        "        except KeyError:\n",
        "            result = 0.0\n",
        "        return result\n",
        "\n",
        "    def random_token(self, context):\n",
        "        \"\"\"\n",
        "        Given a context we \"semi-randomly\" select the next word to append in a sequence\n",
        "        :param context:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        r = random.random()\n",
        "        map_to_probs = {}\n",
        "        token_of_interest = self.context[context]\n",
        "        for token in token_of_interest:\n",
        "            map_to_probs[token] = self.prob(context, token)\n",
        "\n",
        "        summ = 0\n",
        "        for token in sorted(map_to_probs):\n",
        "            summ += map_to_probs[token]\n",
        "            if summ > r:\n",
        "                return token\n",
        "\n",
        "    def generate_text(self, token_count: int):\n",
        "        \"\"\"\n",
        "        :param token_count: number of words to be produced\n",
        "        :return: generated text\n",
        "        \"\"\"\n",
        "        n = self.n\n",
        "        context_queue = (n - 1) * ['<START>']\n",
        "        result = []\n",
        "        for _ in range(token_count):\n",
        "            obj = self.random_token(tuple(context_queue))\n",
        "            result.append(obj)\n",
        "            if n > 1:\n",
        "                context_queue.pop(0)\n",
        "                if obj == '.':\n",
        "                    context_queue = (n - 1) * ['<START>']\n",
        "                else:\n",
        "                    context_queue.append(obj)\n",
        "        return ' '.join(result)\n",
        "\n",
        "\n",
        "def create_ngram_model(n, path):\n",
        "    m = NgramModel(n)\n",
        "    with open(path, 'r') as f:\n",
        "        text = f.read()\n",
        "        text = text.split('.')\n",
        "        for sentence in text:\n",
        "            # add back the fullstop\n",
        "            sentence += '.'\n",
        "            m.update(sentence)\n",
        "    return m\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    m = create_ngram_model(6, '01Genesis.txt')\n",
        "\n",
        "    print (f'Language Model creating time: {time.time() - start}')\n",
        "    start = time.time()\n",
        "    random.seed(7)\n",
        "    print(f'{\"=\"*50}\\nGenerated text:')\n",
        "    print(m.generate_text(20))\n",
        "    print(f'{\"=\"*50}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language Model creating time: 0.26215481758117676\n",
            "==================================================\n",
            "Generated text:\n",
            "29He gave them his last charge and said , ‘I shall soon be gathered to my father’s kin ; bury\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_ebIIoicFlO"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE-mr5XmcF71"
      },
      "source": [
        "Ex 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxmDkZQTcHhP",
        "outputId": "c0b91d66-0c70-49be-901c-f9cfa53f9107"
      },
      "source": [
        "nltk.download(\"brown\")\n",
        "nltk.download(\"universal_tagset\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGhU24XLcSow"
      },
      "source": [
        "rel = nltk.corpus.brown.tagged_words(categories=\"religion\", tagset=\"universal\")\n",
        "rom = nltk.corpus.brown.tagged_words(categories=\"romance\", tagset=\"universal\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWDsYcqQcd_M"
      },
      "source": [
        "rel_clean = sorted(\n",
        "    set([word for (word, tag) in rel if tag == \"NOUN\" and word.isalpha() and not word[0].isupper()])\n",
        ")\n",
        "\n",
        "romn_clean = sorted(\n",
        "    set([word for (word, tag) in rom if tag == \"NOUN\" and word.isalpha() and not word[0].isupper()])\n",
        ")\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIV4uQmPcoXC"
      },
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHFkpqJkdFdb",
        "outputId": "67ac6cbd-f8a3-441b-b3a3-bef2ed3ea9f4"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnOAGhEwcrLf"
      },
      "source": [
        "type = \"n\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaPr8xsYcuQX"
      },
      "source": [
        "synsets = wn.all_synsets(type)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKxCoefydONj"
      },
      "source": [
        "def find_polysemy(text):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for w in text:\n",
        "        count += len(wn.synsets(w))\n",
        "\n",
        "    return count / len(text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-UjnTCBdQ8R",
        "outputId": "be0f9399-b1f3-4caa-9397-c93da2b3a6a1"
      },
      "source": [
        "n_rel = find_polysemy(rel_clean)\n",
        "print(n_rel)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.917355371900826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy6VyeoAdTf1",
        "outputId": "a9136c06-5d95-4200-a6a5-40c58add9de9"
      },
      "source": [
        "n_romn = find_polysemy(romn_clean)\n",
        "print(n_romn)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.017577500798978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k7TEskodq-h"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def find_meronyms(text):\n",
        "\n",
        "    count_part_m = 0\n",
        "    count_subst_m = 0\n",
        "\n",
        "    for w in text:\n",
        "\n",
        "        try:\n",
        "\n",
        "            word = wn.synset(f\"{lemmatizer.lemmatize(w)}.n.01\")\n",
        "\n",
        "            count_part_m += len(word.part_meronyms())\n",
        "            count_subst_m += len(word.substance_meronyms())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    return count_part_m, count_subst_m\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pygKI5Lbd6HC",
        "outputId": "9933dce3-8d3d-4247-8707-b39fc576a517"
      },
      "source": [
        "count_part_m, count_subst_m = find_meronyms(rel_clean)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no lemma 'anchoritism' with part of speech 'n'\n",
            "no lemma 'anyone' with part of speech 'n'\n",
            "no lemma 'anything' with part of speech 'n'\n",
            "no lemma 'bonzes' with part of speech 'n'\n",
            "no lemma 'carryovers' with part of speech 'n'\n",
            "no lemma 'compromising' with part of speech 'n'\n",
            "no lemma 'deadweight' with part of speech 'n'\n",
            "no lemma 'doing' with part of speech 'n'\n",
            "no lemma 'electrodynamics' with part of speech 'n'\n",
            "no lemma 'enjoinder' with part of speech 'n'\n",
            "no lemma 'everybody' with part of speech 'n'\n",
            "no lemma 'everyone' with part of speech 'n'\n",
            "no lemma 'everything' with part of speech 'n'\n",
            "no lemma 'falling' with part of speech 'n'\n",
            "no lemma 'filmstrips' with part of speech 'n'\n",
            "no lemma 'finite' with part of speech 'n'\n",
            "no lemma 'foreshortening' with part of speech 'n'\n",
            "no lemma 'heightening' with part of speech 'n'\n",
            "no lemma 'historicity' with part of speech 'n'\n",
            "no lemma 'indigation' with part of speech 'n'\n",
            "no lemma 'metaphysic' with part of speech 'n'\n",
            "no lemma 'others' with part of speech 'n'\n",
            "no lemma 'revellings' with part of speech 'n'\n",
            "no lemma 'securing' with part of speech 'n'\n",
            "no lemma 'something' with part of speech 'n'\n",
            "no lemma 'supermachine' with part of speech 'n'\n",
            "no lemma 'uncircumcision' with part of speech 'n'\n",
            "no lemma 'unconvincing' with part of speech 'n'\n",
            "no lemma 'undergirding' with part of speech 'n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2wFvFyleANl",
        "outputId": "21dbfb03-e1ac-45df-bc50-5c22b0b9187a"
      },
      "source": [
        "print(count_part_m + count_subst_m)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfXObmseFSS",
        "outputId": "6142ca2c-4453-49b9-d61c-7f95c7984859"
      },
      "source": [
        "count_part_m, count_subst_m = find_meronyms(rel_clean)\n",
        "print(count_part_m + count_subst_m)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no lemma 'anchoritism' with part of speech 'n'\n",
            "no lemma 'anyone' with part of speech 'n'\n",
            "no lemma 'anything' with part of speech 'n'\n",
            "no lemma 'bonzes' with part of speech 'n'\n",
            "no lemma 'carryovers' with part of speech 'n'\n",
            "no lemma 'compromising' with part of speech 'n'\n",
            "no lemma 'deadweight' with part of speech 'n'\n",
            "no lemma 'doing' with part of speech 'n'\n",
            "no lemma 'electrodynamics' with part of speech 'n'\n",
            "no lemma 'enjoinder' with part of speech 'n'\n",
            "no lemma 'everybody' with part of speech 'n'\n",
            "no lemma 'everyone' with part of speech 'n'\n",
            "no lemma 'everything' with part of speech 'n'\n",
            "no lemma 'falling' with part of speech 'n'\n",
            "no lemma 'filmstrips' with part of speech 'n'\n",
            "no lemma 'finite' with part of speech 'n'\n",
            "no lemma 'foreshortening' with part of speech 'n'\n",
            "no lemma 'heightening' with part of speech 'n'\n",
            "no lemma 'historicity' with part of speech 'n'\n",
            "no lemma 'indigation' with part of speech 'n'\n",
            "no lemma 'metaphysic' with part of speech 'n'\n",
            "no lemma 'others' with part of speech 'n'\n",
            "no lemma 'revellings' with part of speech 'n'\n",
            "no lemma 'securing' with part of speech 'n'\n",
            "no lemma 'something' with part of speech 'n'\n",
            "no lemma 'supermachine' with part of speech 'n'\n",
            "no lemma 'uncircumcision' with part of speech 'n'\n",
            "no lemma 'unconvincing' with part of speech 'n'\n",
            "no lemma 'undergirding' with part of speech 'n'\n",
            "786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "aAIpZzVAeNob",
        "outputId": "65a41bd7-d888-4e1d-c9c3-6dfb6bb71d7d"
      },
      "source": [
        "q = \"worries\"\n",
        "tree = wn.synset(f\"{q}.n.01\")\n",
        "\n",
        "tree.part_meronyms()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "WordNetError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynset_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'n'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-99de770db9fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"worries\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{q}.n.01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpart_meronyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no lemma %r with part of speech %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mWordNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mn_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWordNetError\u001b[0m: no lemma 'worries' with part of speech 'n'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1_srSp6eRwY",
        "outputId": "aaac82aa-b962-429e-a1b1-595a5ae6ab3d"
      },
      "source": [
        "wn.synsets(\"worries\", wn.NOUN)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('concern.n.04'), Synset('worry.n.02')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJteC2YSiAf2"
      },
      "source": [
        "Ex 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C39HajCmhc6r"
      },
      "source": [
        "lemmas = []\n",
        "for synset in synsets:\n",
        "  for lemma in synset.lemmas():\n",
        "    lemmas.append(lemma.name())"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa6YwfRLhjGm"
      },
      "source": [
        "lemmas = set(lemmas)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO-QXg2LiHMQ"
      },
      "source": [
        "count = 0\n",
        "for lemma in lemmas:\n",
        "  count = count + len(wn.synsets(lemma, type))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuCAuo0RiOgs",
        "outputId": "044bfce6-00d5-4726-96d9-aca2a56ca4e3"
      },
      "source": [
        "print('Total distinct lemmas: ', len(lemmas))\n",
        "print('Total senses :',count)\n",
        "print('Average Polysemy of ', type,': ' , count/len(lemmas))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total distinct lemmas:  119034\n",
            "Total senses : 152763\n",
            "Average Polysemy of  n :  1.2833560159282222\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}